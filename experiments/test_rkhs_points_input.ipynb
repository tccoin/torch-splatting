{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from rkhs_splatting.utils.camera_utils import parse_camera\n",
    "from rkhs_splatting.rkhs_model import RKHSModel\n",
    "from rkhs_splatting.rkhs_render import RKHSRenderer\n",
    "from rkhs_splatting.rkhs_model_global_scale import RKHSModelGlobalScale\n",
    "from rkhs_splatting.rkhs_render_global_scale import RKHSRendererGlobalScale\n",
    "from rkhs_splatting.utils.dataloader import TartanAirLoader\n",
    "\n",
    "import datetime\n",
    "import pathlib\n",
    "from icecream import ic\n",
    "from spatialmath import SE3\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly_utils import *\n",
    "from test_utils import *\n",
    "from experiments.points_trainer import GSSTrainer\n",
    "\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "global scale\n",
    "\"\"\"\n",
    "\n",
    "#config\n",
    "input_source = 'sample' # tartanair, sample, manual, random\n",
    "scale = 0.5\n",
    "input_initial_scaling = scale\n",
    "map_initial_scaling = scale\n",
    "map_minimum_scaling = scale\n",
    "radii_multiplier = 6\n",
    "scale_trainable = False\n",
    "tile_size = 64\n",
    "\n",
    "# input\n",
    "train_pcs = []\n",
    "cameras = []\n",
    "if input_source=='sample':\n",
    "    train_pcs, cameras = load_sample_dataset('../data/B075X65R3X', [0,1], 0.5)\n",
    "elif input_source=='tartanair':\n",
    "    dataset = TartanAirLoader('/home/junzhewu/dataset/rzh/tartanair/soulcity/Easy/P001')\n",
    "    train_pcs, cameras = load_custom_dataset(dataset, [0,1,30], 0.5)\n",
    "    tile_size = 80\n",
    "elif input_source=='manual':\n",
    "    train_pcs = [create_pc(np.array([\n",
    "        [1,1,0, 1,0,0,1],\n",
    "        [-1,-1,0, 1,0,0,1],\n",
    "    ]))]\n",
    "elif input_source=='random':\n",
    "    train_pcs = [create_random_pc(n=100, mu=[0,0,0], sigma=5, rgba=np.array([1,0,0,1]), shape='line')]\n",
    "\n",
    "# initial map\n",
    "# init_map = create_pc(np.array([\n",
    "#     [1.5,1.5,0, 1,0,0,1],\n",
    "#     [-1.5,-1.5,0, 1,0,0,1],\n",
    "#     [0,0,0, 1,0,0,1],\n",
    "#     [0,10,0, 1,0,0,1],\n",
    "# ]))\n",
    "# init_map = create_random_pc(n=2**10, mu=0, sigma=1, rgba=np.array([1,0,0,1]))\n",
    "# init_map = train_pcs[0].random_sample(2**14)\n",
    "init_map = train_pcs[0].generate_random_noise(2**12)\n",
    "# init_map = train_pcs[0].generate_random_color(2**10)\n",
    "\n",
    "# cameras\n",
    "if len(cameras)==0:\n",
    "    camera_intrinsic = [355, 355, 128, 128] # fx, fy, cx, cy\n",
    "    n_cameras = 1\n",
    "    delta_deg = 30.0\n",
    "    camera_c2w_init = SE3.Tz(-10)\n",
    "    for i in range(n_cameras):\n",
    "        camera_c2w = SE3.Ry(delta_deg*i / 180 * np.pi)@camera_c2w_init\n",
    "        camera_data = create_camera(*camera_intrinsic, camera_c2w)\n",
    "        cameras.append(to_viewpoint_camera(camera_data))\n",
    "\n",
    "# render\n",
    "map_model = RKHSModelGlobalScale(sh_degree=4, debug=False, trainable=True, scale_trainable=scale_trainable)\n",
    "map_model.create_from_pcd(init_map, initial_scaling=map_initial_scaling)\n",
    "input_model = RKHSModelGlobalScale(sh_degree=4, debug=False, trainable=False)\n",
    "renderer = RKHSRendererGlobalScale(white_bkgd=True)\n",
    "\n",
    "input_frames = []\n",
    "for i, camera in enumerate(cameras):\n",
    "    # single frame or multiple frames\n",
    "    if len(train_pcs)==1:\n",
    "        if i==0:\n",
    "            input_model.create_from_pcd(train_pcs[0], initial_scaling=input_initial_scaling)\n",
    "    else:\n",
    "        input_model.create_from_pcd(train_pcs[i], initial_scaling=input_initial_scaling)\n",
    "    # render\n",
    "    input_frame = renderer(\n",
    "        camera,\n",
    "        input_model.get_xyz,\n",
    "        input_model.get_opacity,\n",
    "        input_model.get_scaling,\n",
    "        input_model.get_features,\n",
    "        radii_multiplier=radii_multiplier,\n",
    "        tile_size=tile_size\n",
    "    )\n",
    "    input_frames.append(input_frame)\n",
    "\n",
    "\n",
    "# plot\n",
    "# fig = go.Figure()\n",
    "# fig.update_layout(scene=dict(aspectmode='data'))\n",
    "# for i in range(len(cameras)):\n",
    "#     camera_c2w = cameras[i].c2w.cpu().detach().numpy()\n",
    "#     plot_camera(fig, camera_c2w[:3,:3], camera_c2w[:3,3], 3, f'camera {i}', True)\n",
    "\n",
    "#     train_pc = train_pcs[i].random_sample(2**12)\n",
    "#     plot_pc(fig, train_pc, 'train_pc', marker_line_width=2, marker_size=5)\n",
    "#     map_pc = init_map\n",
    "#     plot_pc(fig, map_pc, 'map_pc', marker_line_width=0, marker_size=5)\n",
    "# fig.show()\n",
    "\n",
    "# show_image(dataset.read_current_rgbd()[0])\n",
    "# show_image(dataset.read_current_rgbd()[1].clip(0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junzhewu/.local/lib/python3.9/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 1 has a total capacity of 47.46 GiB of which 51.38 MiB is free. Process 4108312 has 32.36 GiB memory in use. Process 340930 has 1.32 GiB memory in use. Process 341935 has 1.32 GiB memory in use. Process 344152 has 1.32 GiB memory in use. Process 376334 has 5.21 GiB memory in use. Process 546274 has 5.88 GiB memory in use. Of the allocated memory 4.75 GiB is allocated by PyTorch, and 270.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m      6\u001b[0m n_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GSSTrainer(\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmap_model,\n\u001b[1;32m     10\u001b[0m     input_model\u001b[38;5;241m=\u001b[39minput_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     writer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_evaluate_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# ic(input_model.get_xyz, input_model.get_features)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# ic(map_model.get_xyz, map_model.get_features)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# ic(init_map.coords, init_map.select_channels(['R', 'G', 'B', 'A']))\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/torch-splatting/experiments/../experiments/points_trainer.py:165\u001b[0m, in \u001b[0;36mGSSTrainer.on_evaluate_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     depth \u001b[38;5;241m=\u001b[39m input_frame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# mask = (input_frame['alpha'] < 0.5).detach().cpu().numpy()\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgauss_render\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcamera\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_xyz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_opacity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mradii_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradii_multiplier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtile_size\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m rgb_pd \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    175\u001b[0m depth_pd \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/torch-splatting/experiments/../rkhs_splatting/rkhs_render_global_scale.py:321\u001b[0m, in \u001b[0;36mRKHSRendererGlobalScale.forward\u001b[0;34m(self, camera, means3d, opacity, scale3d, features, mode, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m prof(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 321\u001b[0m         rets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcamera\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcamera\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpoint_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoint_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmeans3d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeans3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale3d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale3d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmeans2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeans2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopacity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopacity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdepths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m            \u001b[49m\u001b[43mradii_multiplier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mradii_multiplier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtile_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtile_size\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    335\u001b[0m         rets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender(\n\u001b[1;32m    336\u001b[0m             camera\u001b[38;5;241m=\u001b[39mcamera,\n\u001b[1;32m    337\u001b[0m             point_ids\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoint_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m             tile_size\u001b[38;5;241m=\u001b[39mtile_size\n\u001b[1;32m    348\u001b[0m         )\n",
      "File \u001b[0;32m~/projects/torch-splatting/experiments/../rkhs_splatting/rkhs_render_global_scale.py:246\u001b[0m, in \u001b[0;36mRKHSRendererGlobalScale.render\u001b[0;34m(self, camera, point_ids, means3d, scale3d, means2d, scale2d, color, opacity, depths, radii_multiplier, tiles_only, tile_size)\u001b[0m\n\u001b[1;32m    244\u001b[0m dx \u001b[38;5;241m=\u001b[39m (tile_coord[:,\u001b[38;5;28;01mNone\u001b[39;00m,:] \u001b[38;5;241m-\u001b[39m sorted_means2D[\u001b[38;5;28;01mNone\u001b[39;00m,:]) \u001b[38;5;66;03m# B P 2\u001b[39;00m\n\u001b[1;32m    245\u001b[0m gauss_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m dx\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39msorted_scale2d_squared)\n\u001b[0;32m--> 246\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mgauss_weight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msorted_opacity\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B P 1\u001b[39;00m\n\u001b[1;32m    247\u001b[0m T \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mones_like(alpha[:,:\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39malpha[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcumprod(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    248\u001b[0m acc_alpha \u001b[38;5;241m=\u001b[39m (alpha \u001b[38;5;241m*\u001b[39m T)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 1 has a total capacity of 47.46 GiB of which 51.38 MiB is free. Process 4108312 has 32.36 GiB memory in use. Process 340930 has 1.32 GiB memory in use. Process 341935 has 1.32 GiB memory in use. Process 344152 has 1.32 GiB memory in use. Process 376334 has 5.21 GiB memory in use. Process 546274 has 5.88 GiB memory in use. Of the allocated memory 4.75 GiB is allocated by PyTorch, and 270.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "folder_name = datetime.datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "folder_name = 'test'\n",
    "results_folder = pathlib.Path('../result/'+folder_name)\n",
    "results_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "n_train = 1000\n",
    "\n",
    "trainer = GSSTrainer(\n",
    "    model=map_model,\n",
    "    input_model=input_model,\n",
    "    renderer=renderer,\n",
    "    # data=data,\n",
    "    use_input_frames=True,\n",
    "    input_frames=input_frames,\n",
    "    train_batch_size=1, \n",
    "    train_num_steps=n_train,\n",
    "    i_image=n_train//40,\n",
    "    train_lr=1e-2,#3e-3\n",
    "    amp=True,\n",
    "    fp16=False,\n",
    "    results_folder=results_folder,\n",
    "    use_rkhs_rgb=True,\n",
    "    use_rkhs_geo=True,\n",
    "    min_scale=map_minimum_scaling,\n",
    "    radii_multiplier=radii_multiplier,\n",
    "    tile_size=tile_size,\n",
    "    writer=False\n",
    ")\n",
    "\n",
    "trainer.on_evaluate_step()\n",
    "trainer.train()\n",
    "\n",
    "# ic(input_model.get_xyz, input_model.get_features)\n",
    "# ic(map_model.get_xyz, map_model.get_features)\n",
    "# ic(init_map.coords, init_map.select_channels(['R', 'G', 'B', 'A']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.update_layout(scene=dict(aspectmode='cube'))\n",
    "train_pc = train_pcs[0].random_sample(2**14)\n",
    "plot_pc(fig, train_pc, 'train_pc', marker_line_width=0, marker_line_color='black', marker_size=1)\n",
    "plot_pc(fig, map_model.to_pc(), 'rkhs_map', marker_line_width=0, marker_line_color='yellow', marker_size=1)\n",
    "# fig.update_scenes(aspectratio=dict(x=1, y=0.001, z=0.001))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
